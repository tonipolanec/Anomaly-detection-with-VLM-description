{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12844781,"sourceType":"datasetVersion","datasetId":8124019},{"sourceId":12915374,"sourceType":"datasetVersion","datasetId":8172234},{"sourceId":12916533,"sourceType":"datasetVersion","datasetId":8173032}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install -q git+https://github.com/huggingface/peft.git transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nimport os\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nset_seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset, Image\nimport json\nfrom PIL import Image as PILImage\n\n\ndef load_custom_dataset(jsonl_path, base_dir):\n    \"\"\"\n    Custom function to load the dataset with full control\n    \"\"\"\n    data = [] \n    with open(jsonl_path, 'r') as f:\n        for line in f:\n            item = json.loads(line.strip())\n            # Make sure image paths are absolute\n            item['image'] = os.path.join(base_dir, item['image'])\n            data.append(item)\n    \n    dataset = Dataset.from_list(data)\n    dataset = dataset.cast_column('image', Image())\n    \n    return dataset\n\ndataset_path = '/kaggle/input/polanecvlm'\njsonl_path = os.path.join(dataset_path, 'dataset.jsonl')\n\ndataset = load_custom_dataset(jsonl_path, dataset_path)\n\n\nprint(dataset[0]['text'])   \nprint(dataset[0]['image'])\ndataset[0]['image']  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nPROMPT = \"Question: What kind of anomaly is on this object? Answer:\"\n\nclass ImageDataset(Dataset):\n    def __init__(self, dataset, processor, prompt = PROMPT):\n        self.dataset = dataset\n        self.processor = processor\n        self.prompt = prompt\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        \n        encoding = self.processor(images=item[\"image\"], padding=\"max_length\", return_tensors=\"pt\")\n        encoding = {k: v.squeeze() for k, v in encoding.items()}\n        \n        encoding[\"text\"] = self.prompt + item[\"text\"]\n        \n        return encoding\n\ndef collate_fn(batch):\n    \n    processed_batch = {}\n    for key in batch[0].keys():\n        if key != \"text\":\n            processed_batch[key] = torch.stack([example[key] for example in batch])\n        else:\n            text_inputs = processor.tokenizer(\n                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n            )\n            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n    return processed_batch\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor, Blip2ForConditionalGeneration, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\n\nprocessor = AutoProcessor.from_pretrained(\n                \"Salesforce/blip2-opt-2.7b\",\n                use_fast=True)\n\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n                \"Salesforce/blip2-opt-2.7b\", \n                device_map = \"auto\", \n                torch_dtype = torch.float16)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"fc1\", \"fc2\"]\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n\ntrain_split_dataset = dataset_split[\"train\"]\ntest_split_dataset  = dataset_split[\"test\"]\n\ntrain_dataset = ImageDataset(train_split_dataset, processor)\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4, collate_fn=collate_fn)\n\ntest_dataset = ImageDataset(test_split_dataset, processor)\ntest_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=1, collate_fn=collate_fn)\n\nprint('Dataloaders setup!')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm \nfrom transformers import get_scheduler\n\nNUM_EPOCHS = 10\nLEARNING_RATE = 1e-4\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n\nnum_training_steps = NUM_EPOCHS * len(train_dataloader)\nwarmup_steps = 0.03 * num_training_steps\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=num_training_steps,\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntraining_losses = []\n\nmodel.train()\nfor epoch in range(NUM_EPOCHS):\n\n    progress_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch+1}/{NUM_EPOCHS}')\n    \n    epoch_losses = []\n    for idx, batch in enumerate(progress_bar):\n\n        input_ids = batch.pop(\"input_ids\").to(device)\n        pixel_values = batch.pop(\"pixel_values\").to(device, torch.float16)\n        attention_mask = batch.pop(\"attention_mask\").to(device)\n    \n        outputs = model(input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        pixel_values=pixel_values,\n                        labels=input_ids)\n        \n        loss = outputs.loss\n        epoch_losses.append(loss.item())\n    \n        loss.backward()\n   \n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n\n        \n    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n    training_losses.append(avg_epoch_loss)\n    print(f'Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}')\n\nprint(\"Training completed!\")\nprint(f\"Final average loss: {training_losses[-1]:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"index = 10\n\nexample_text = test_split_dataset[index]['text']\nexample_image = test_split_dataset[index]['image']\n\nprint(example_text)   \nexample_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving model on HuggingFace\n\n```python\nfrom huggingface_hub import notebook_login\nnotebook_login()\n```\n---\n```python\nmodel.push_to_hub(\"tonipol/blip2-opt-2.7b-anomaly-detection-description\")\n```","metadata":{}},{"cell_type":"markdown","source":"## How to load from HuggingFace\n\n```python\nfrom transformers import Blip2ForConditionalGeneration, AutoProcessor\nfrom peft import PeftModel, PeftConfig\n\npeft_model_id = \"tonipol/blip2-opt-2.7b-anomaly-detection-description\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\nmodel = Blip2ForConditionalGeneration.from_pretrained(config.base_model_name_or_path, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model, peft_model_id)\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n```","metadata":{}},{"cell_type":"markdown","source":"## How to generate captions\n\n```python\ninputs = processor(images=img, text=PROMPT, return_tensors=\"pt\").to(device, torch.float16)\npixel_values = inputs.pixel_values\n\ngenerated_ids = model.generate(\n    pixel_values=pixel_values, \n    max_length=50,\n    temperature=0.7,  \n    top_p=0.8,\n    do_sample=True\n    )\ngenerated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n```","metadata":{}}]}